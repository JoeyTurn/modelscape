import numpy as np
import torch
import inspect
import torch.nn as nn
import torch.optim as optim
from mupify import mupify, rescale
from modelscape.model import centeredmodel
from modelscape.backend.utils import _extract_kwargs_for

def _resolve_cls(spec, module):
    """
    Turn 'Adam' -> torch.optim.Adam, or just pass through callables.
    """
    if isinstance(spec, str):
        try:
            return getattr(module, spec)
        except AttributeError:
            raise ValueError(f"{spec!r} not found in {module.__name__}")
    if callable(spec):
        return spec
    raise TypeError(f"optimizer/loss spec must be str or callable, got {type(spec)}")


def train_model(model, batch_function, lr=1e-2, max_iter=int(1e3), loss_checkpoints=None, percent_thresholds=None,
                  gamma=1., ema_smoother=0.0, X_tr=None, y_tr=None, X_te=None, y_te=None, only_thresholds=False,
                  verbose=False, optimizer="SGD", loss="MSELoss", mup_param="mup", **kwargs):
    """
    Returns:
        dict of model, train_losses, test_losses, timekeys, others

    - timekeys[j]: first gradient step where the loss drops below the j-th threshold.
    - If RELATIVE mode, thresholds are `percent_thresholds * init_loss`.
    - If ABSOLUTE mode, thresholds are raw absolutes and the comparison metric is raw loss.
    - If only_thresholds is True, only returns the timekeys and not the full loss curves.
    """

    def return_statement(model, ema_tr, ema_te, timekeys, tr_losses=None, te_losses=None, extras={}):
        if only_thresholds:
            extras.update({"model": model, "train_losses": ema_tr, "test_losses": ema_te, "timekeys": timekeys})
            return extras
        # if otherreturns is not None:
        #     for name, _ in items:
        #         extras[name] = np.array(extras[name])
        extras.update({"model": model, "train_losses": tr_losses, "test_losses": te_losses, "timekeys": timekeys})
        return extras 

    def fill_losses(tr_losses, te_losses, extras, i):
        tr_losses[i:] = tr_losses[i]
        te_losses[i:] = te_losses[i]
        if otherreturns is not None:
            for name, _ in list(otherreturns.items()):
                extras[name][i:] = extras[name][i]
        return tr_losses, te_losses, extras

    # Convert float-like values to lists
    if loss_checkpoints is not None and not isinstance(loss_checkpoints, (list, tuple, np.ndarray)):
        loss_checkpoints = np.array([loss_checkpoints])
    if percent_thresholds is not None and not isinstance(percent_thresholds, (list, tuple, np.ndarray)):
        percent_thresholds = np.array([percent_thresholds])
    
    #checking if losses are absolute or relative
    has_abs = (loss_checkpoints is not None) and len(loss_checkpoints) > 0
    is_relative = (percent_thresholds is not None) and len(percent_thresholds) > 0
    if has_abs and is_relative or not has_abs and not is_relative:
        raise ValueError("Provide exactly one of loss_checkpoints OR percent_thresholds.")

    optimizer_cls = (
        kwargs.pop("OPTIMIZER_CLASS", None)
        or kwargs.pop("optimizer_class", None)
        or kwargs.pop("OPTIMIZER_FN", None)
        or kwargs.pop("optimizer_fn", None)
    )
    optimizer_instance = (
        kwargs.pop("OPTIMIZER_INSTANCE", None)
        or kwargs.pop("optimizer_instance", None)
    )

    if optimizer_instance is None:
        opt_cls = optimizer_cls if optimizer_cls is not None else _resolve_cls(optimizer, optim)
        opt_kwargs, kwargs = _extract_kwargs_for(opt_cls, kwargs)
    else:
        opt_cls = None
    loss_cls = None
    loss_kwargs = {}
    loss_class = (
        kwargs.pop("LOSS_CLASS", None)
        or kwargs.pop("loss_class", None)
        or kwargs.pop("LOSS_FN", None)
        or kwargs.pop("loss_fn", None)
    )
    loss_instance = (
        kwargs.pop("LOSS_INSTANCE", None)
        or kwargs.pop("loss_instance", None)
    )
    if loss_instance is None:
        loss_cls = loss_class if loss_class is not None else _resolve_cls(loss, nn)
        loss_kwargs, kwargs = _extract_kwargs_for(loss_cls, kwargs)

    # model stuff
    if optimizer_instance is None:
        lr = lr * gamma if gamma >= 1 else lr * (gamma**2.)
        opt_kwargs.setdefault("lr", lr)
        opt = opt_cls(model.parameters(), **opt_kwargs)
    else:
        opt = optimizer_instance
    # opt = torch.optim.SGD(model.parameters(), lr=lr)
    mupify(model, opt, param=mup_param)
    rescale(model, gamma)
    model = centeredmodel(model).to(next(model.parameters()).device)
    if loss_instance is None:
        loss_fn = loss_cls(**loss_kwargs)
    else:
        loss_fn = loss_instance
    # loss_fn = torch.nn.MSELoss()

    # thresholding
    thresholds = np.asarray(percent_thresholds if is_relative else loss_checkpoints, dtype=float)
    thresholds = np.sort(thresholds)[::-1] # descending
    timekeys = np.full(thresholds.shape, 0, dtype=int)

    otherreturns = kwargs.get("otherreturns", None)
    if type(otherreturns) is dict:
        items = list(otherreturns.items())
    extras = {}

    if not(only_thresholds):
        tr_losses = np.empty(max_iter, dtype=float)
        te_losses = np.empty(max_iter, dtype=float)
        # if otherreturns is not None:
        #     for name, _ in items:
                # extras[name] = np.empty((max_iter, 200, 200), dtype=float) #temporary, this is gonna need to be CHANGED
                # extras[name] = []#np.empty(max_iter, dtype=float)
    else:
        tr_losses = te_losses = None
        if otherreturns is not None:
            for name, _ in items:
                extras[name] = None
    ema = None
    pointer = 0   
    X_tr_not_provided = X_tr is None

    # training loop 
    # print("Starting training loop...")
    for i in range(int(max_iter)):
        X_tr, y_tr = batch_function(i)
        
        opt.zero_grad()
        out = model(X_tr).squeeze()
        loss = loss_fn(out, y_tr.squeeze())
        loss.backward()
        opt.step()

        tr_loss_val = float(loss.item())
        if X_tr_not_provided:
            te_loss_val = tr_loss_val
        else:
            with torch.no_grad():
                out = model(X_te).squeeze()
                loss = loss_fn(out, y_te.squeeze())
                te_loss_val = float(loss.item())
        # initialize thresholds & loss trace baseline at first step
        if i == 0:
            ema_tr = tr_loss_val
            ema_te = te_loss_val
            if is_relative:
                thresholds *= tr_loss_val
            # prefill losses after init val calculated
            if not(only_thresholds):
                if otherreturns is not None:
                    for name, fn in items:
                        val = np.array(fn(model=model.model, X_tr=X_tr, y_tr=y_tr, X_te=X_te, y_te=y_te, opt=opt, **kwargs)) #the model.model to unwrap from centeredmodel
                        shape = (max_iter,) + val.shape
                        extras[name] = np.empty(shape, dtype=val.dtype)
                        # extras[name][:]= val
                tr_losses, te_losses, extras=fill_losses(tr_losses, te_losses, extras, i)

        ema_tr = (ema_smoother * ema_tr + (1.0 - ema_smoother) * tr_loss_val)
        ema_te = (ema_smoother * ema_te + (1.0 - ema_smoother) * te_loss_val)
        if not(only_thresholds):
            tr_losses[i] = ema_tr
            te_losses[i] = ema_te
            if otherreturns is not None:
                for name, fn in items:
                    val = fn(model=model.model, X_tr=X_tr, y_tr=y_tr, X_te=X_te, y_te=y_te, opt=opt, **kwargs) #the model.model to unwrap from centeredmodel
                    extras[name][i] = val
                    
        if verbose:
            print(f"Step {i}: ema train loss {ema_tr}, ema test loss {ema_te}")

        # if a threshold is hit, update pointer
        while pointer < len(thresholds) and ema_tr < thresholds[pointer]:
            timekeys[pointer] = i
            if not(only_thresholds):
                tr_losses, te_losses, extras=fill_losses(tr_losses, te_losses, extras, i)
            pointer += 1

        # early exit if all thresholds crossed
        if pointer == len(thresholds):
            if otherreturns is not None and only_thresholds:
                for name, fn in items:
                    val = fn(model=model.model, X_tr=X_tr, y_tr=y_tr, X_te=X_te, y_te=y_te, opt=opt, **kwargs)
                    extras[name] = val
            return return_statement(model, ema_tr, ema_te, timekeys, tr_losses, te_losses, extras)


    if i == max_iter - 1:
        if otherreturns is not None and only_thresholds:
            for name, fn in items:
                val = fn(model=model.model, X_tr=X_tr, y_tr=y_tr, X_te=X_te, y_te=y_te, opt=opt, **kwargs)
                extras[name] = val
    return return_statement(model, ema_tr, ema_te, timekeys, tr_losses, te_losses, extras)
